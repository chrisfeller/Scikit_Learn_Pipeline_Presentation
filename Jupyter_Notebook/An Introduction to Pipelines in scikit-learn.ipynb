{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Introduction to Pipelines in Scikit-Learn\n",
    "### Chris Feller | Galvanize Data Science Immersive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, Some Background on Scikit-Learn's API Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main objects you will work with in scikit-learn:\n",
    "    <br>\n",
    "1. **Estimators**: An object that can estimate some parameters based on a dataset. An estimator learns from the data. This is the `.fit()` step. Example: `StandardScaler()` is an estimator since it takes in a dataset and calculates or 'learns' the dataset's parameters (e.g., mean and standard deviation). \n",
    "    <br>\n",
    "2. **Transformers**: An object that changes the data in some way. This is the `.transform()` step. Example: `Imputer()` is a transformer because it imputes null values, thus changing the underlying data. It is important to note that some estimators (such as `StandardScaler()`) can also transform a datasest.\n",
    "    <br>\n",
    "3. **Predictors**: An object that is capable of making predictions on a given dataset. Takes a dataset of new instances and returns a dataset of corresponding predictions. This is the `.predict()` step. Also has a `.score()` method to measure the quality of the predictions given a test set.\n",
    "\n",
    "\n",
    "The general process of scikit-learn is:\n",
    "```\n",
    ".fit()\n",
    ".transform()\n",
    ".predict()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So What Are Pipelines and Why Are They Useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, this `.fit()`, `.transform()`, `.predict()` process happens in sequence, but in separate self-contained steps. However, Pipelines make it so that we can chain these steps together into a single unit that still executes in sequential order. In essence, a Pipeline is a way to chain multiple estimators, transformers, and predictors into a single unit. \n",
    "\n",
    "This is useful for a multitude of reasons:\n",
    "1. Readability\n",
    "    - The intent of the code is clearer and the code is more readable\n",
    "2. Efficiency\n",
    "    - You only make one `fit()` and `predict()` call on the entire sequence instead of at each step\n",
    "    - Leads to a faster modeling loop and easier experimentation \n",
    "    - It makes it trivial to move ordering of the Pipeline pieces, or to swap pieces in and out\n",
    "3. Safety\n",
    "    - Ensures that each transformation of the data is being performed in the correct order, which protects from inadvertent data leakage\n",
    "    - You don't have to keep track of data during intermediate steps\n",
    "4. Joint Parameter Selection\n",
    "    - You can grid-search once over all parameters of all your transformers and estimators at once\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Do They Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pipeline is built using a list of `(key, value)` tuples, where the `key` is a string containing the name you want to give the step and `value` is an estimator object. The names of each step can be anything you'd like as long as they don't include any double underscores `__`. Here's a general outline:\n",
    "```\n",
    "pipeline = Pipeline([\n",
    "    ('name', estimator()),\n",
    "    ('name', transformer(), \n",
    "    ('name', predictor()),\n",
    "    ])\n",
    "```\n",
    "All estimators in a pipeline, except for the last one, must be transformers (i.e., they take X, do something to X, and then spit out a transformed X). In other words, each of the estimators, except for the last one, must implement a `fit()` and `transform()` method. The last estimator can be of any type (transformer, predictor, etc.). The Pipeline as a whole has all of the methods that the last estimator in the pipeline has, i.e., if the last estimator is a predictor such as Logistic Regression then the entire pipeline has a `fit()`, `transform()`, and `predict()` method. If instead the last estimator is a transformer, then the pipeline as a whole would not have a `predict()` method.  \n",
    "\n",
    "Calling `fit()` on the pipeline is the same as calling `fit()` on each estimator in turn, `transform()` the input and pass it on to the next step. In other words, when you call the pipeline's `fit()` method, it calls `fit_transform()` sequentially on all transformers, passing the output of each call as the parameter to the next call, until it reaches the final estimator, for which it just calls the `fit()` or `predict()` method.\n",
    "\n",
    "\n",
    "Example in which we are imputing the median for missing values, standardizing the data, and then building a Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', Imputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "    ('logistic_model', LogisticRegression()),\n",
    "    ])\n",
    "\n",
    "# Create Fake Data\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                             n_clusters_per_class=1)\n",
    "# Perform Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Fit and Predict on Pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureUnions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some situations, you'll need to combine the output of two separate pipelines into one. For example, you may have a pandas DataFrame which requires separate processing for numeric features and categorical features. You could create a Pipeline to deal with the numeric features and a separate Pipeline to deal with the categorical features and then combine the two prior to fitting a model. `FeatureUnion` does this seamlessly, by taking multiple estimators which return columns in parallel and concatenating their results together using `np.hstack`. In practice, Pipelines and FeatureUnions are commonly used in conjunction to create complex workflows. \n",
    "\n",
    "`Pipeline` chains things together in sequential order, while `FeatureUnion` chains things together in parallel. `Pipeline` items need to happen in order, while `FeatureUnion` items can happen at the same time.\n",
    "\n",
    "A few things to keep in mind when using `FeatureUnion`:\n",
    "* A FeatureUnion has no way of checking whether two transformers might produce identical features. It only produces a union when the feature sets are disjoint, and making sure they are the caller's responsibility). So the output of your FeatureUnion may have duplicate features if you're not careful. \n",
    "* For some complex transformers, alignment may be tricky. Pandas is good at this, but sometimes runs into trouble in FeatureUnions because `np.hstack` is called, which ignores indexes.\n",
    "* Writing a generalizable transformer often means you will expect the correct column to be selected from your X matrix, oftentimes this means writing a custom selector (via Custom Transformers which we'll get to next), which is too bad.\n",
    "\n",
    "Example in which we take an array of random numbers and then concatenate two additional arrays to it, one with the square-root of the original array and one with the square of the original array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.15427374e-01   8.45829400e-01   5.11836328e-01]\n",
      " [  5.20721797e-01   7.21610558e-01   2.71151190e-01]\n",
      " [  6.99023359e-01   8.36076168e-01   4.88633656e-01]\n",
      " [  6.94400787e-01   8.33307138e-01   4.82192453e-01]\n",
      " [  1.10036141e-02   1.04898113e-01   1.21079523e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Create Fake Data\n",
    "X = np.random.random((5,1))\n",
    "\n",
    "feat_union = FeatureUnion([('identity',FunctionTransformer()),\n",
    "                    ('sqrt',FunctionTransformer(np.sqrt)),\n",
    "                    ('square', FunctionTransformer(lambda x: x**2))])\n",
    "\n",
    "print(feat_union.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often during preprocessing and feature selection, we write our own functions that transform the data (e.g. drop columns, multiply two columns together, etc.). And although scikit-learn provides many useful transformers, such as `StandardScaler()`, that do much of this preprocessing for us, you will need to write your own for tasks such as custom cleanup operations. As a reminder, a transformer is just an object that responds to `fit()`, `transform()`, and `fit_transform()`. A transformer can be thought of as a data in, data out black box.\n",
    "\n",
    "To create a custom transformer that will fit into a Pipeline, all you need is to create a class and implement three methods:\n",
    "1. `.fit()`  - which returns `self`\n",
    "2. `.transform()`\n",
    "3. `.fit_transform()`\n",
    "    - You can get this last one for free by simply adding `TransformerMixin` as a base class.\n",
    "    - Similarly, if you use `BaseEstimator` as a base class (and avoid `*args` and `**kwargs` in your constructor) your transformer will have grid-searchable parameters, which will be very useful later on. \n",
    "\n",
    "\n",
    "**Custom Transformer Template**:\n",
    "```\n",
    "class MyTransformer(TransformerMixin, BaseEstimator):\n",
    "    '''A template for a custom transformer.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # transform X via code or additional methods\n",
    "        return X\n",
    "```\n",
    "\n",
    "* `fit()` **ALWAYS** returns `self`. Sometimes it can set state variables if you will need those to transform test data later on. Otherwise it just does nothing. Either way, it returns `self`.\n",
    "* Even though your `fit()` method isn't doing anything with `y`, it still needs to be a parameter. Just set it to `None`. On the flip side, the `transform` method **ONLY** takes `X` and won't work if you include a `y`.\n",
    "* `transform()` is where most of the transformations happen! In this method, `X` is transformed somehow (perhaps through other methods within the transformer), and then the transformed `X` is returned.\n",
    "\n",
    "\n",
    "Example in which we implement our own version of `StandardScaler()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class MyScaler(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Scale to zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Recommended signature for custom transformer's\n",
    "        fit method.\n",
    "\n",
    "        Set state in your transformer with whatever information\n",
    "        is needed to transform later.\n",
    "        \"\"\"\n",
    "        #You have to return self, so we can chain!\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.scale = np.std(X, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Recommended signature for custom transformer's\n",
    "        transform method.\n",
    "\n",
    "        Use state (if any) to transform some X data. This X\n",
    "        may be the same X passed to fit, but it may also be new data,\n",
    "        as in the case of a CV dataset. Both are treated the same.\n",
    "        \"\"\"\n",
    "        #Do transformations\n",
    "        Xt = X.copy()\n",
    "        Xt -= self.mean\n",
    "        Xt /= self.scale\n",
    "        return Xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearching with Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best features of Pipelines is you can pass your entire pipeline object into GridSearchCV to optimize your hyperparameters. By combining GridSearchCV with Pipeline you can also cross-validate and optimize any upstream transformers. This could come in handy if you were doing dimensionality reduction before classifying, and wanted to compare techniques as seen in the example below:\n",
    "    \n",
    "```\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('clf', SVC()),\n",
    "    ])\n",
    "\n",
    "param_grid = {'reduce_dim__n_components:[2, 5, 10], \n",
    "              'clf__C:[0.1, 10, 100]}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid)\n",
    "```\n",
    "\n",
    "Note that when building the `param_grid` dictionary to gridsearch over, the naming convention is as follows; The name of the pipeline step (e.g., `clf`), followed by two underscores, followed by the name of the parameter. For example, below `'my_classifier` is the name of the random forest classifier in the pipeline and `min_samples_split` is the parameter we are trying to optimize.\n",
    "```\n",
    "pipeline = Pipeline([(“my_classifier”, RandomForestClassifier())])\n",
    "parameters = {my_classifier__min_samples_split=[2, 3, 4, 5]}   \n",
    "cv = GridSearchCV(pipeline, param_grid = parameters)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples Using the Titanic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare an entire process with and without Pipelines to see their true value in action. Using the Titanic dataset, we'll build a Random Forest Classifier to predict which passengers survived the historic shipwreck. In the process, we'll impute values for missing data, standardize numeric features, one hot encode categorical features, and gridsearch to find the optimal parameters in the final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Non-Pipeline Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Accuracy:  0.811659192825\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/titanic.csv')\n",
    "\n",
    "# Select features to use in our final model\n",
    "df_features = df[['Sex', 'Embarked', 'Pclass', 'Fare', 'Age', 'SibSp', 'Parch', 'Survived']]\n",
    "\n",
    "# Perform a train/test split \n",
    "y = df_features.pop('Survived')\n",
    "X = df_features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "# Impute missing values w/ the mean of the numeric features and the mode of categorical features\n",
    "X_train['Fare'].fillna(X_test['Fare'].median(), inplace=True)\n",
    "X_train['Age'].fillna(X_test['Age'].median(), inplace=True)\n",
    "X_train['Embarked'].fillna(X_test['Embarked'].mode(), inplace=True)\n",
    "\n",
    "X_test['Fare'].fillna(X_test['Fare'].median(), inplace=True)\n",
    "X_test['Age'].fillna(X_test['Age'].median(), inplace=True)\n",
    "X_test['Embarked'].fillna(X_test['Embarked'].mode(), inplace=True)\n",
    "\n",
    "# One hot encode categorical features\n",
    "for col in ['Sex', 'Embarked', 'Pclass']:\n",
    "    X_train = pd.concat([X_train, pd.get_dummies(X_train[col], prefix=col,\n",
    "                        drop_first=True)], axis=1)\n",
    "    X_train.drop(col, inplace=True, axis=1)\n",
    "    X_test = pd.concat([X_test, pd.get_dummies(X_test[col],\n",
    "                        prefix=col, drop_first=True)], axis=1)\n",
    "    X_test.drop(col, inplace=True, axis=1)\n",
    "\n",
    "# Standardize the numeric features\n",
    "features_to_scale = ['Fare', 'Age', 'SibSp', 'Parch']\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[features_to_scale])\n",
    "X_train[features_to_scale] = scaler.transform(X_train[features_to_scale])\n",
    "X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "# Gridsearch on random forest classifier\n",
    "random_forest_param_list = {\"max_depth\": [None],\n",
    "          \"max_features\": np.arange(1,9, 1),\n",
    "          \"min_samples_split\": np.arange(2,100,10),\n",
    "          \"min_samples_leaf\": np.arange(2,100, 100),\n",
    "          \"bootstrap\": [True, False],\n",
    "          \"n_estimators\" :[100,200, 300],\n",
    "          \"criterion\": [\"gini\"]}\n",
    "rf = RandomForestClassifier()\n",
    "g = GridSearchCV(rf, random_forest_param_list, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "g.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = g.predict(X_test)\n",
    "print('Final Model Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pipeline Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take advantage of all that Pipelines have to offer. We'll start by creating three custom transformers. After performing a train/test split we'll create two separate pipelines, one for numerical features and another for categorical features. We'll then create one final pipeline, which includes a FeatureUnion to combine the outputs of the first two pipelines. We'll gridsearch on that final pipeline and make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Accuracy:  0.784753363229\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Build Custom Transformers\n",
    "class CustomSelector(TransformerMixin, BaseEstimator):\n",
    "    '''Custom Transformer to select categorical or numerical features'''\n",
    "\n",
    "    def __init__(self, categorical=True):\n",
    "        self.categorical = categorical\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.categorical:\n",
    "            return X[['Sex', 'Embarked', 'Pclass']]\n",
    "        else:\n",
    "            return X[['Fare', 'Age', 'SibSp', 'Parch']]\n",
    "        \n",
    "class CustomCategoricalImputer(TransformerMixin, BaseEstimator):\n",
    "    '''Custom Transformer to impute the most frequent value in categorical features'''\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.fill = X.mode().iloc[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.fillna(self.fill)\n",
    "    \n",
    "class CustomCategoricalEncoder(TransformerMixin, BaseEstimator):\n",
    "    '''Custom Transformer to implement One Hot Encoding on categorical features'''\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        for col in ['Sex', 'Embarked', 'Pclass']:\n",
    "            X = pd.concat([X, pd.get_dummies(X[col], prefix=col,\n",
    "                        drop_first=True)], axis=1)\n",
    "            X.drop(col, inplace=True, axis=1)\n",
    "        return X\n",
    "    \n",
    "# Load data\n",
    "df = pd.read_csv('../data/titanic.csv')\n",
    "\n",
    "# Select features to use in our final model\n",
    "df_features = df[['Sex', 'Embarked', 'Pclass', 'Fare', 'Age', 'SibSp', 'Parch', 'Survived']]\n",
    "\n",
    "# Perform train/test split\n",
    "y = df_features.pop('Survived')\n",
    "X = df_features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "# Build Pipeline for numerical features\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', CustomSelector(categorical=False)),\n",
    "    ('imputer', Imputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Build Pipeline for categorical features\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', CustomSelector(categorical=True)),\n",
    "    ('imputer', CustomCategoricalImputer()),\n",
    "    ('cat_encoder', CustomCategoricalEncoder())\n",
    "])\n",
    "\n",
    "# Build Full Pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('feat_union', FeatureUnion(transformer_list=[\n",
    "    ('num_pipeline', num_pipeline),\n",
    "    ('cat_pipeline', cat_pipeline),\n",
    "    ])),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Gridsearch on entire pipeline\n",
    "random_forest_param_list = {\"rf__max_depth\": [None],\n",
    "          \"rf__max_features\": np.arange(1,9, 1),\n",
    "          \"rf__min_samples_split\": np.arange(2,100,10),\n",
    "          \"rf__min_samples_leaf\": np.arange(2,100, 100),\n",
    "          \"rf__bootstrap\": [True, False],\n",
    "          \"rf__n_estimators\" :[100,200, 300],\n",
    "          \"rf__criterion\": [\"gini\"]}\n",
    "\n",
    "grid = GridSearchCV(full_pipeline, param_grid=random_forest_param_list, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = grid.predict(X_test)\n",
    "print('Final Model Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
